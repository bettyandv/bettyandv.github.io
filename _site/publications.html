<!DOCTYPE html>
<html lang="en">
<!-- Template: https://github.com/luost26/academic-homepage -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Publications - Ye Bai</title>

    <!-- Stylesheets -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/css/bootstrap.min.css" integrity="sha512-P5MgMn1jBN01asBgU0z60Qk4QxiXo86+wlFahKrsQf37c9cro517WzVSPPV1tDKzhku2iJ2FVgL67wG03SGnNA==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css" integrity="sha512-b1ASx0WHgVFL5ZQhTgiPWX+68KjS38Jk87jg7pe+qC7q9YkEtFq0z7xCglv7qGIs/68d3mAp+StfC8WKC5SSAg==" crossorigin="anonymous" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&family=Fira+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
    <link rel="stylesheet" href="/assets/css/global.css">
</head>
<body class="bg-light" data-spy="scroll" data-target="#navbar-year" data-offset="100">
    <nav class="navbar navbar-expand-sm navbar-dark bg-dark fixed-top mb-5 shadow-sm">
    <div class="container-lg">
        <a class="navbar-brand"><strong>Ye Bai</strong></a>
        <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fas fa-map"></i> Menu
        </button>

        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="/">Home</a>
                </li>
                
                <li class="nav-item active">
                    <a class="nav-link" href="/publications">Publications</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>

    <div class="container-lg">
        

<div class="row">
    <div class="col-12 col-lg-10">
        
        
        <h2 class="pt-4" id="year-2025">2025</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-xl">
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/assets/images/covers/maple_overview.png" alt="MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table Reasoning" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table Reasoning</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Ye Bai</strong>, </span><a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=F6nm6awAAAAJ&hl=en/">Minghan Wang</a>, <a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=cx2eAe0AAAAJ&hl=en/">Thuy-Trang Vu</a></p>
            <p class="mt-0 mb-0 small"><i>The 23rd Annual Workshop of the Australasian Language Technology Association (ALTA 2025)</i> 2025  <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">Table-based question answering requires complex reasoning capabilities that current LLMs struggle to achieve with single-pass inference. Existing approaches, such as Chain-of-Thought reasoning and question decomposition, lack error detection mechanisms and discard problem-solving experiences, contrasting sharply with how humans tackle such problems. In this paper, we propose MAPLE (Multi-agent Adaptive Planning with Long-term mEmory), a novel framework that mimics human problem-solving through specialized cognitive agents working in a feedback-driven loop. MAPLE integrates 4 key components: (1) a Solver using the ReAct paradigm for reasoning, (2) a Checker for answer verification, (3) a Reflector for error diagnosis and strategy correction, and (4) an Archiver managing long-term memory for experience reuse and evolution. Experiments on WiKiTQ and TabFact demonstrate significant improvements over existing methods, achieving state-of-the-art performance across multiple LLM backbones.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://aclanthology.org/2025.alta-main.10/">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/bettyandv/MAPLE-table-reasoning">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray rounded-xl-top  lazy" data-src="/assets/images/covers/maple_overview.png">
    <div class="w-100 rounded-xl-top " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table Reasoning</h5>
                <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Ye Bai</strong>, </span><a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=F6nm6awAAAAJ&hl=en/">Minghan Wang</a>, <a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=cx2eAe0AAAAJ&hl=en/">Thuy-Trang Vu</a></p>
                <p class="mt-0 mb-0 small"><i>The 23rd Annual Workshop of the Australasian Language Technology Association (ALTA 2025)</i> 2025  <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">Table-based question answering requires complex reasoning capabilities that current LLMs struggle to achieve with single-pass inference. Existing approaches, such as Chain-of-Thought reasoning and question decomposition, lack error detection mechanisms and discard problem-solving experiences, contrasting sharply with how humans tackle such problems. In this paper, we propose MAPLE (Multi-agent Adaptive Planning with Long-term mEmory), a novel framework that mimics human problem-solving through specialized cognitive agents working in a feedback-driven loop. MAPLE integrates 4 key components: (1) a Solver using the ReAct paradigm for reasoning, (2) a Checker for answer verification, (3) a Reflector for error diagnosis and strategy correction, and (4) an Archiver managing long-term memory for experience reuse and evolution. Experiments on WiKiTQ and TabFact demonstrate significant improvements over existing methods, achieving state-of-the-art performance across multiple LLM backbones.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2025.alta-main.10/">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/bettyandv/MAPLE-table-reasoning">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters border-bottom border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/assets/images/covers/time_overview.png" alt="Discrete Minds in a Continuous World: Do Language Models Know Time Passes?" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">Discrete Minds in a Continuous World: Do Language Models Know Time Passes?</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=F6nm6awAAAAJ&hl=en/">Minghan Wang</a>, <span class="text-body">
            <strong>Ye Bai</strong>, </span><a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=cx2eAe0AAAAJ&hl=en/">Thuy-Trang Vu</a>, <a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=EhnQJFwAAAAJ&hl=en/">Ehsan Shareghi</a>, <a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=Perjx5EAAAAJ&hl=en/">Gholamreza Haffari</a></p>
            <p class="mt-0 mb-0 small"><i>Conference on Empirical Methods in Natural Language Processing (EMNLP)</i> 2025  <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">While Large Language Models (LLMs) excel at temporal reasoning tasks like event ordering and duration estimation, their ability to perceive the actual passage of time remains unexplored. We investigate whether LLMs perceive the passage of time and adapt their decision-making accordingly through three complementary experiments. First, we introduce the Token-Time Hypothesis, positing that LLMs can map discrete token counts to continuous wall-clock time, and validate this through a dialogue duration judgment task. Second, we demonstrate that LLMs could use this awareness to adapt their response length while maintaining accuracy when users express urgency in question answering tasks. Finally, we develop BombRush, an interactive navigation challenge that examines how LLMs modify behavior under progressive time pressure in dynamic environments. Our findings indicate that LLMs possess certain awareness of time passage, enabling them to bridge discrete linguistic tokens and continuous physical time, though this capability varies with model size and reasoning abilities. This work establishes a theoretical foundation for enhancing temporal awareness in LLMs for time-sensitive applications.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://aclanthology.org/2025.findings-emnlp.1016/">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/yuriak/LLMTimePerception/tree/main">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none border-bottom border-gray   lazy" data-src="/assets/images/covers/time_overview.png">
    <div class="w-100  " style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">Discrete Minds in a Continuous World: Do Language Models Know Time Passes?</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=F6nm6awAAAAJ&hl=en/">Minghan Wang</a>, <span class="text-body">
            <strong>Ye Bai</strong>, </span><a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=cx2eAe0AAAAJ&hl=en/">Thuy-Trang Vu</a>, <a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=EhnQJFwAAAAJ&hl=en/">Ehsan Shareghi</a>, <a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=Perjx5EAAAAJ&hl=en/">Gholamreza Haffari</a></p>
                <p class="mt-0 mb-0 small"><i>Conference on Empirical Methods in Natural Language Processing (EMNLP)</i> 2025  <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">While Large Language Models (LLMs) excel at temporal reasoning tasks like event ordering and duration estimation, their ability to perceive the actual passage of time remains unexplored. We investigate whether LLMs perceive the passage of time and adapt their decision-making accordingly through three complementary experiments. First, we introduce the Token-Time Hypothesis, positing that LLMs can map discrete token counts to continuous wall-clock time, and validate this through a dialogue duration judgment task. Second, we demonstrate that LLMs could use this awareness to adapt their response length while maintaining accuracy when users express urgency in question answering tasks. Finally, we develop BombRush, an interactive navigation challenge that examines how LLMs modify behavior under progressive time pressure in dynamic environments. Our findings indicate that LLMs possess certain awareness of time passage, enabling them to bridge discrete linguistic tokens and continuous physical time, though this capability varies with model size and reasoning abilities. This work establishes a theoretical foundation for enhancing temporal awareness in LLMs for time-sensitive applications.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://aclanthology.org/2025.findings-emnlp.1016/">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/yuriak/LLMTimePerception/tree/main">[Code]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
                
<div class="d-none d-md-block">
    <div class="row no-gutters  border-gray">
        <div class="col-md-3 col-xl-2 mb-md-0 p-md-3"><img data-src="/assets/images/covers/sdf_framework.png" alt="SpeechDialogueFactory: A Framework for Natural Speech Dialogue Generation" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0">
            <h5 class="mt-0 mb-1 font-weight-normal">SpeechDialogueFactory: A Framework for Natural Speech Dialogue Generation</h5>
            <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=F6nm6awAAAAJ&hl=en/">Minghan Wang</a>, <span class="text-body">
            <strong>Ye Bai</strong>, </span><a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=8MshEYoAAAAJ&hl=en/">Yuxia Wang</a>, <a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=cx2eAe0AAAAJ&hl=en/">Thuy-Trang Vu</a>, <a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=EhnQJFwAAAAJ&hl=en/">Ehsan Shareghi</a>, <a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=Perjx5EAAAAJ&hl=en/">Gholamreza Haffari</a></p>
            <p class="mt-0 mb-0 small"><i>Interspeech</i> 2025  <span data-semantic-scholar-id=""></span></p>
            <p class="mt-0 mb-0 small text-muted">High-quality speech dialogue datasets are crucial for Speech-LLM development, yet existing acquisition methods face significant limitations. Human recordings incur high costs and privacy concerns, while synthetic approaches often lack conversational authenticity. To address these challenges, we introduce \textsc{SpeechDialogueFactory}, a production-ready framework for generating natural speech dialogues efficiently. Our solution employs a comprehensive pipeline including metadata generation, dialogue scripting, paralinguistic-enriched utterance simulation, and natural speech synthesis with voice cloning. Additionally, the system provides an interactive UI for detailed sample inspection and a high-throughput batch synthesis mode. Evaluations show that dialogues generated by our system achieve a quality comparable to human recordings while significantly reducing production costs. We release our work as an open-source toolkit, alongside example datasets available in English and Chinese, empowering researchers and developers in Speech-LLM research and development.</p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://www.isca-archive.org/interspeech_2025/wang25x_interspeech.pdf">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/yuriak/SpeechDialogueFactory">[Code]</a>
                
                
                
                <a target="_blank" href="https://huggingface.co/datasets/minghanw/sdf_dataset_zh">[Project Page]</a>
                
                
            </p>

        </div>
    </div>
</div>

<div class="row no-gutters d-md-none  border-gray  rounded-xl-bottom lazy" data-src="/assets/images/covers/sdf_framework.png">
    <div class="w-100  rounded-xl-bottom" style="background-color: rgba(255,255,255,0.9);">
        <div class="d-flex align-items-start flex-column py-3 px-4">
            <div class="mb-auto"></div>
            <div>
                <h5 class="mt-0 mb-1 font-weight-normal">SpeechDialogueFactory: A Framework for Natural Speech Dialogue Generation</h5>
                <p class="mt-0 mb-0 small"><a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=F6nm6awAAAAJ&hl=en/">Minghan Wang</a>, <span class="text-body">
            <strong>Ye Bai</strong>, </span><a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=8MshEYoAAAAJ&hl=en/">Yuxia Wang</a>, <a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=cx2eAe0AAAAJ&hl=en/">Thuy-Trang Vu</a>, <a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=EhnQJFwAAAAJ&hl=en/">Ehsan Shareghi</a>, <a class="text-body" target="_blank" href="https://scholar.google.com/citations?user=Perjx5EAAAAJ&hl=en/">Gholamreza Haffari</a></p>
                <p class="mt-0 mb-0 small"><i>Interspeech</i> 2025  <span data-semantic-scholar-id=""></span></p>
                <p class="mt-0 mb-0 small text-muted">High-quality speech dialogue datasets are crucial for Speech-LLM development, yet existing acquisition methods face significant limitations. Human recordings incur high costs and privacy concerns, while synthetic approaches often lack conversational authenticity. To address these challenges, we introduce \textsc{SpeechDialogueFactory}, a production-ready framework for generating natural speech dialogues efficiently. Our solution employs a comprehensive pipeline including metadata generation, dialogue scripting, paralinguistic-enriched utterance simulation, and natural speech synthesis with voice cloning. Additionally, the system provides an interactive UI for detailed sample inspection and a high-throughput batch synthesis mode. Evaluations show that dialogues generated by our system achieve a quality comparable to human recordings while significantly reducing production costs. We release our work as an open-source toolkit, alongside example datasets available in English and Chinese, empowering researchers and developers in Speech-LLM research and development.</p>
                
                <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                    
                    
                    <a target="_blank" href="https://www.isca-archive.org/interspeech_2025/wang25x_interspeech.pdf">[Paper]</a>
                    
                    
                    
                    <a target="_blank" href="https://github.com/yuriak/SpeechDialogueFactory">[Code]</a>
                    
                    
                    
                    <a target="_blank" href="https://huggingface.co/datasets/minghanw/sdf_dataset_zh">[Project Page]</a>
                    
                    
                </p>
            </div>
        </div>
    </div>

</div>
            
        </div>
        
    </div>

    <div class="col-2 d-none d-lg-block">
        <div id="navbar-year" class="nav nav-pills flex-column sticky-top" style="top: 80px">
            
            <a class="nav-link d-block" href="#year-2025">2025</a>
            
        </div>
    </div>

</div>

    </div>
    <footer class="footer border-top py-2 mt-5 bg-white small">
    <div class="container-lg">
        <div class="row my-3">
            <div class="col-6">
                <div class="text-muted">
                    <i>Last updated: Dec 2025</i>
                </div>
            </div>
            <div class="col-6">
                <div class="text-right text-muted">
                    <a href="https://github.com/luost26/academic-homepage" target="_blank"><i class="fas fa-pencil-ruler"></i> academic-homepage</a>
                </div>
            </div>
        </div>
    </div>
</footer>


    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.lazy/1.7.9/jquery.lazy.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/js/bootstrap.min.js" integrity="sha512-XKa9Hemdy1Ui3KSGgJdgMyYlUg1gM+QhL6cnlyTe2qzMCYm4nAZ1PsVerQzTTXzonUR+dmswHqgJPuwCq1MaAg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/github-buttons/2.14.2/buttons.min.js" integrity="sha512-OYwZx04hKFeFNYrWxIyo3atgGpb+cxU0ENWBZs72X7T9U+NoHPM1ftUn/Mfw7dRDXrqWA6M1wBg6z6fGE32aeA==" crossorigin="anonymous"></script>
    <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
    <script src="https://unpkg.com/imagesloaded@5/imagesloaded.pkgd.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false}
              ],
              throwOnError : false
            });
        });
    </script>
    <script src="/assets/js/common.js"></script>
    <script src="/assets/js/bubble_visual_hash.js"></script>
    <script src="/assets/js/semantic_scholar_citation_count.js"></script>
</body>
</html>
